---
title: "Modelos base"
author: "Catalina Riquelme"
date: "2023-05-09"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    collapsed: true
    smooth_scroll: true
    theme: journal
    highlight: kate
    df_print: paged
    code_folding: show
---
- Finalidad: Realizar un modelo base que incluya todos los datos (SNP's) para cada uno de los fenotipos.
- Número de SNP's: 54
- Algotirmos: Random Forest, Support Vector Machine, K-nearest neighbors y Ridge

Se entrena cuatro modelos de regresión por cada fenotipo, uno con cada algoritmo, es por esto que los modelos de 
regresión son generados en cuatro procesos distintos, uno por cada algotimo. En cada uno de estos procesos se generan
modelos para cada uno de los fenotipos con un algoritmo en particular, a través de ciclos, los cuales repiten las 
mismas etapas para cada uno de los fenotipos. Al final de estos procesos se alamcenan sus métricas de evaluación 
para una futura evaluación.


# Procesos modelos de regresión

## Transformación datos categoricos
Debido a que los valores de cada uno de los SNP's son bases nitrogenadas se deben transformar los datos a variables
dummy para poder ser utilizadas en el modelo de regresión.

## División datos
Al ser un conjunto de datos reducidos se utiliza dividen los datos en 70% entrenamiento y un 30% de prueba.

## Entrenamiento
Se realiza un entrenamiento de los modelos con los diferentes algoritmos, tomando los datos de entrenamiento 
(70% de los datos totales) con una **validación cruzada simple** debido a la cantidad reducida de datos, evitando 
el sobreajuste o subajuste de los datos, a través de la creación de varias particiones de los datos, entrenando 
y probando el modelo con cada una de ellas, para esta ocasión se utilizan 10 particiones.

### Coeficiente de determinación (R^2^)
Medida estádistica que refelja la relación entre las variables y qué tan bien predice los valores de la variable 
independiente. Se aproxima el resultado con 4 dígitos. Su ecuación es:
$$R^2 = \frac{\sum_{t=1}^{T}(y_t - \hat{y_t})^2}{\sum_{t=1}^{T}(y_t - \bar{y})^2}$$

### Error cuadrático medio (MSE)
Utilizado para evaluar la calidad de un modelo de regresión. Indicando cuanto se desvia el modelo generado de los datos 
observados. Su ecuación es:

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2$$

# Preprocesamiento de datos 

## Librerias

```{r message=FALSE, warning=FALSE}
library(readxl)
library(randomForest)
library(caret)
library(dplyr)
library(tibble)
library(openxlsx)
library(vioplot)
cat("\014")
```

## Lectura de los datos

```{r message=FALSE, warning=FALSE}
fenotipos_path <- "C:/Users/ca_re/OneDrive/Desktop/Modelos/fenotipos.xlsx"
lectura_datos <- read_excel(fenotipos_path, 
                            col_types = c(rep("text", 55), rep("numeric", 12)),
                            na = "NA")
nombres_fenotipos <- names(lectura_datos)[56:67]
nombres_fenotipos
```
# Random Forest
Algoritmo de aprendizaje automático que se utiliza tanto para problemas de clasificación como para problemas de 
regresión, construye varios árboles basados en muestras aleatorias. Cada árbol predice valores y, al promediar estas
predicciones, se obtiene una estimación final más sólida y menos propensa a errores individuales.
```{r}
resultados_rf <- data.frame(matrix(ncol=3,nrow=0)) # Dataframe donde se guardaran los resultados
for(i in 1:length(nombres_fenotipos)){
  fenotipo <- nombres_fenotipos[i]
  datos <- setNames(data.frame(lectura_datos[[fenotipo]],lectura_datos[, 2:55]), 
                    c(fenotipo, names(lectura_datos[, 2:55])))
  
  # Se eliminan filas con fatos NA
  datos <- datos[-which(is.na(datos[[fenotipo]])),]
  
  # Transformación datos categoricos
  datos_dummy <- dummyVars(as.formula(paste(fenotipo, "~ .")), data = datos)
  datos_dummy <- predict(datos_dummy, newdata = datos)
  datos_dummy <- data.frame(datos_dummy,fenotipo=datos[,1])
  nombres_genes <- colnames(datos_dummy)
  
  # División datos
  set.seed(10)
  datos_train_index <- sample(1:nrow(datos_dummy), 0.7 * nrow(datos_dummy))
  datos_train <- datos_dummy[datos_train_index, ]
  datos_test <- datos_dummy[-datos_train_index, ]
  
  model <- train(fenotipo ~ ., 
                 data = datos_train, 
                 method = "rf", 
                 trControl = trainControl(method = "cv", number = 10))
  
  importancia <- varImp(model)
  importancia
  
  plot(importancia, main = paste("Importancia de variables", nombres_fenotipos[i],"Ridge"),
       xlab = "Variables", ylab = "Importancia",top = 10)

  # Evaluación
  
  # Predicciones
  predictions <- predict(model, newdata = datos_test)
  
  # Coeficiente de determinación (R^2)
  r2 = round(cor(datos_test$fenotipo,predictions)^2,8) 
  
  # Error cuadrático medio (MSE)
  mse = round(mean((datos_test$fenotipo - predictions)^2),8) 
  evaluacion <- data.frame(Modelo = "ModeloBase_rf", Fenotipo= nombres_fenotipos[i]
                           ,R2 = r2, MSE = mse)
  resultados_rf <- rbind(resultados_rf, evaluacion)

}
resultados_rf
```


# Support Vector Machine
Algoritmo de aprendizaje automático, que busca encontrar una línea o hiperplano que mejor se ajuste a los datos,
minimizando la distancia entre el hiperplano y los puntos de datos. Se busca maximizar el margen entre los puntos 
más cercanos al hiperplano, permitiendo así predecir valores numéricos basados en nuevas observaciones.
```{r}
resultados_svm <- data.frame(matrix(ncol=3,nrow=0)) # Dataframe donde se guardaran los resultados
for(i in 1:length(nombres_fenotipos)){
  fenotipo <- nombres_fenotipos[i]
  datos <- setNames(data.frame(lectura_datos[[fenotipo]],lectura_datos[, 2:55]), 
                    c(fenotipo, names(lectura_datos[, 2:55])))
  # Se eliminan filas con fatos NA
  datos <- datos[-which(is.na(datos[[fenotipo]])),]
  
  # Transformación datos categoricos
  datos_dummy <- dummyVars(as.formula(paste(fenotipo, "~ .")), data = datos)
  datos_dummy <- predict(datos_dummy, newdata = datos)
  datos_dummy <- data.frame(datos_dummy,fenotipo=datos[,1])
  nombres_genes <- colnames(datos_dummy)
  
  # División datos
  set.seed(100)
  datos_train_index <- sample(1:nrow(datos_dummy), 0.7 * nrow(datos_dummy))
  datos_train <- datos_dummy[datos_train_index, ]
  datos_test <- datos_dummy[-datos_train_index, ]
  
  model <- train(fenotipo ~ ., 
                 data = datos_train, 
                 method = "svmLinear", 
                 trControl = trainControl(method = "cv", number = 10))
  
  # Evaluación
  
  # Predicciones
  predictions <- predict(model, newdata = datos_test)
  
  # Coeficiente de determinación (R^2)
  r2 = round(cor(datos_test$fenotipo,predictions)^2,8) 
  
  # Error cuadrático medio (MSE)
  mse = round(mean((datos_test$fenotipo - predictions)^2),8) 
  evaluacion <- data.frame(Modelo = "ModeloBase_svm", Fenotipo= nombres_fenotipos[i]
                           , R2 = r2, MSE = mse)
  resultados_svm <- rbind(resultados_svm, evaluacion)

}
resultados_svm
```

# K-nearest neighbors
Algoritmo de aprendizaje automático, se basa en encontrar una instancia basándose en los valores de sus vecinos más 
cercanos en un espacio de características. El valor se calcula promediando los valores de los k vecinos más cercanos. 
Es simple pero puede sufrir de sensibilidad a datos ruidosos y alta carga computacional en conjuntos de datos grandes.
```{r}
resultados_knn <- data.frame(matrix(ncol=3,nrow=0)) # Dataframe donde se guardaran los resultados
for(i in 1:length(nombres_fenotipos)){
  fenotipo <- nombres_fenotipos[i]
  datos <- setNames(data.frame(lectura_datos[[fenotipo]],lectura_datos[, 2:55]), 
                    c(fenotipo, names(lectura_datos[, 2:55])))
  
  #Se eliminan filas con fatos NA
  datos <- datos[-which(is.na(datos[[fenotipo]])),]
  
  # Transformación datos categoricos
  datos_dummy <- dummyVars(as.formula(paste(fenotipo, "~ .")), data = datos)
  datos_dummy <- predict(datos_dummy, newdata = datos)
  datos_dummy <- data.frame(datos_dummy,fenotipo=datos[,1])
  nombres_genes <- colnames(datos_dummy)
  
  # División datos
  set.seed(1000)
  datos_train_index <- sample(1:nrow(datos_dummy), 0.7 * nrow(datos_dummy))
  datos_train <- datos_dummy[datos_train_index, ]
  datos_test <- datos_dummy[-datos_train_index, ]
  
  model <- train(fenotipo ~ ., 
                 data = datos_train, 
                 method = "knn", 
                 trControl = trainControl(method = "cv", number = 10))
  
  # Evaluación
  
  # Predicciones
  predictions <- predict(model, newdata = datos_test)
  
  # Coeficiente de determinación (R^2)
  r2 = round(cor(datos_test$fenotipo,predictions)^2,8) 
  
  # Error cuadrático medio (MSE)
  mse = round(mean((datos_test$fenotipo - predictions)^2),8) 
  
  evaluacion <- data.frame(Modelo = "ModeloBase_knn", Fenotipo= nombres_fenotipos[i]
                           , R2 = r2, MSE = mse)
  resultados_knn <- rbind(resultados_knn, evaluacion)

}
resultados_knn
```

# Ridge
Método de regresión que introduce una penalización en los coeficientes de las características, forzándolos a ser 
pequeños y evitando valores extremos. Esto reduce la complejidad del modelo y mejora su generalización a datos nuevos. 
La regularización de Ridge controla la magnitud de los coeficientes, evitando que se vuelvan dominantes y ofreciendo un 
equilibrio entre ajuste y simplicidad.

```{r}
resultados_ridge <- data.frame(matrix(ncol=3,nrow=0)) # Dataframe donde se guardaran los resultados
for(i in 1:length(nombres_fenotipos)){
  fenotipo <- nombres_fenotipos[i]
  datos <- setNames(data.frame(lectura_datos[[fenotipo]],lectura_datos[, 2:55]), 
                    c(fenotipo, names(lectura_datos[, 2:55])))
  #Se eliminan filas con fatos NA
  datos <- datos[-which(is.na(datos[[fenotipo]])),]
  
  ## Transformación datos categoricos
  datos_dummy <- dummyVars(as.formula(paste(fenotipo, "~ .")), data = datos)
  datos_dummy <- predict(datos_dummy, newdata = datos)
  datos_dummy <- data.frame(datos_dummy,fenotipo=datos[,1])
  nombres_genes <- colnames(datos_dummy)
  
  # División datos
  set.seed(10000)
  datos_train_index <- sample(1:nrow(datos_dummy), 0.7 * nrow(datos_dummy))
  datos_train <- datos_dummy[datos_train_index, ]
  datos_test <- datos_dummy[-datos_train_index, ]
  
  model <- train(fenotipo ~ ., 
                 data = datos_train, 
                 method = 'glmnet',
                 tuneGrid = expand.grid(alpha = 0, lambda = 1),
                 trControl = trainControl(method = "cv", number = 10))

  # Evaluación
  
  # Predicciones
  predictions <- predict(model, newdata = datos_test)
  
  # Coeficiente de determinación (R^2)
  r2 = round(cor(datos_test$fenotipo,predictions)^2,8) 
  
  # Error cuadrático medio (MSE)
  mse = round(mean((datos_test$fenotipo - predictions)^2),8) 
  
  evaluacion <- data.frame(Modelo = "ModeloBase_ridge", Fenotipo= nombres_fenotipos[i]
                           , R2 = r2, MSE = mse)
  resultados_ridge <- rbind(resultados_ridge, evaluacion)

}
resultados_ridge
```

# Resultados
Se unen los resultados obtenidos de los diversos algoritmos.
```{r}
resultados <- rbind(resultados_rf, resultados_svm,resultados_knn,resultados_ridge)
resultados
```

